import os
import logging
import datetime
from elasticsearch import Elasticsearch

# Hardcoded file path
FILE_PATH = '/datafiles/infadata/scripts/yourfile.txt'
LOG_PATH_TEMPLATE = '/datafiles/infadata/elastic/Scripts/log/script_{}.log'

# Configure logging based on current date
current_date = datetime.datetime.now().strftime('%Y%m%d')
LOG_PATH = LOG_PATH_TEMPLATE.format(current_date)

if not os.path.exists(os.path.dirname(LOG_PATH)):
    os.makedirs(os.path.dirname(LOG_PATH))

logging.basicConfig(filename=LOG_PATH, level=logging.INFO,
                    format='%(asctime)s %(levelname)s:%(message)s')

def delete_old_logs(log_dir, days=90):
    """Delete log files older than 'days' days."""
    now = datetime.datetime.now()
    cutoff = now - datetime.timedelta(days=days)

    for filename in os.listdir(log_dir):
        file_path = os.path.join(log_dir, filename)
        if os.path.isfile(file_path):
            file_mtime = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))
            if file_mtime < cutoff:
                os.remove(file_path)
                logging.info(f"Deleted old log file: {file_path}")

def read_file(file_path):
    customer_ids = []
    errors = []
    with open(file_path, 'r') as file:
        for line_num, line in enumerate(file, start=1):
            parts = line.strip().split(',')
            for custid in parts:
                custid = custid.strip()
                if not custid.isdigit():
                    error_msg = f"Invalid custid (not an integer) at line {line_num}: {custid}"
                    logging.error(error_msg)
                    errors.append(error_msg)
                    continue
                customer_ids.append(custid)
                print(f"CustomerID to be anonymized: {custid}")
    
    if errors:
        logging.error("The following errors were found in the input file:")
        for error in errors:
            logging.error(error)
        raise ValueError("Invalid records found in the input file. Check log for details.")
    
    return customer_ids

def get_indices_with_customerid_field(es):
    all_indices = es.indices.get_alias().keys()
    indices_with_customerid = []

    for index in all_indices:
        mapping = es.indices.get_mapping(index=index)
        properties = mapping[index]['mappings'].get('properties', {})
        if 'CustomerID' in properties:
            indices_with_customerid.append(index)
    
    return indices_with_customerid

def search_and_update_in_elasticsearch(es, indices, custid):
    query = {
        "bool": {
            "must": [
                {"term": {"CustomerID": custid}}
            ]
        }
    }

    update_script = {
        "script": {
            "source": """
                ctx._source.FirstName = 'Anonymous';
                ctx._source.LastName = 'Anonymous' + params.custid;
                ctx._source.MiddleName = '';
                ctx._source.EmailAddress = 'Anonymous';
                ctx._source.ValidEmailAddress = 'Anonymous';
                ctx._source.AddressLine1 = 'Anonymous';
            """,
            "lang": "painless",
            "params": {
                "custid": custid
            }
        },
        "query": query
    }

    matched_info = {}
    unmatched_info = []
    updated_indices = []
    for index in indices:
        result = es.search(index=index, body={"query": query, "size": 1000})
        hits = result['hits']['hits']
        total_matched = len(hits)
        
        if total_matched > 0:
            # Update all matched documents
            es.update_by_query(index=index, body=update_script)
            print(f"Updated {total_matched} documents in index '{index}' for CustomerID '{custid}'")

            matched_info[index] = {
                'total_matched': total_matched
            }
            updated_indices.append(index)
        else:
            unmatched_info.append(index)

    return matched_info, unmatched_info, updated_indices

def main():
    # Elasticsearch host
    ES_HOST = 'http://localhost:9200'
    es = Elasticsearch([ES_HOST])
    
    if not es.ping():
        logging.error("Elasticsearch connection failed.")
        raise SystemExit("Elasticsearch connection failed. Exiting the script.")
    else:
        logging.info("Elasticsearch is connected.")

    if not os.path.exists(FILE_PATH):
        logging.error("File not found at path: " + FILE_PATH)
        raise SystemExit("File not found at path: " + FILE_PATH)

    customer_ids = read_file(FILE_PATH)
    indices = get_indices_with_customerid_field(es)

    if not indices:
        logging.error("No indices found with the CustomerID field.")
        raise SystemExit("No indices found with the CustomerID field. Exiting the script.")

    # Print indices that will be scanned
    print("Below indices will be scanned:")
    for index in indices:
        print(index)
    
    total_indices_scanned = 0
    total_docs_updated = 0
    total_records_processed = 0
    total_indices_updated = set()

    for custid in customer_ids:
        total_records_processed += 1
        matched_info, unmatched_info, updated_indices = search_and_update_in_elasticsearch(es, indices, custid)
        total_indices_scanned += len(indices)

        print(f"Scanned indices for CustomerID {custid}: {indices}")
        logging.info(f"Scanned indices for CustomerID {custid}: {indices}")

        if matched_info:
            for index, info in matched_info.items():
                total_docs_up
